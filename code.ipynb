{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIgM6C9HYUhm"
   },
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "Name: Liana Mardanova\n",
    "e-mail: l.mardanova@innopolis.university\n",
    "Group: DS-01\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification of Decisions\n",
    "\n",
    "### 1. Comparison with Norvig’s Spell Corrector: Need for Context-Aware Correction\n",
    "\n",
    "One of the main reasons for moving away from **Norvig’s spell corrector** is its use of a **unigram-based approach**, which treats each word independently. This approach limits the ability to handle contextual relationships in sentences. Below is a comparison between Norvig’s model and my context-aware model:\n",
    "\n",
    "#### Test Sentences:\n",
    "\n",
    "```python\n",
    "test_sentences = [\n",
    "    \"a bad cas of the\",\n",
    "    \"He fel in a puddle.\",\n",
    "    \"He did not fel well.\"\n",
    "]\n",
    "```\n",
    "\n",
    "#### Norvig’s Output:\n",
    "\n",
    "Original: a bad cas of the  \n",
    "Corrected: a bad was of the  \n",
    "\n",
    "Original: He fel in a puddle.  \n",
    "Corrected: he few in a puddles  \n",
    "\n",
    "Original: He did not fel well.  \n",
    "Corrected: he did not few well  \n",
    "\n",
    "\n",
    "In these examples, Norvig makes incorrect corrections:\n",
    "- \"cas\" is changed to \"was\" instead of \"case\".\n",
    "- \"fel\" is corrected to \"few\", which is incorrect in this context.\n",
    "- \"fel well\" is corrected to \"few well\", which makes no sense grammatically or semantically.\n",
    "\n",
    "#### My Model’s Output:\n",
    "\n",
    "Original: a bad cas of the  \n",
    "Corrected: a bad case of the  \n",
    "\n",
    "Original: He fel in a puddle.  \n",
    "Corrected: he fell in a puddle .  \n",
    "\n",
    "Original: He did not fel well.  \n",
    "Corrected: he did not feel well .  \n",
    "\n",
    "\n",
    "In contrast, my model correctly identifies that:\n",
    "- \"cas\" should be corrected to \"case\", maintaining the meaning of the sentence.\n",
    "- \"fel\" should be corrected to \"fell\" or \"feel\" depending on the context, improving the grammatical correctness and semantic meaning of the sentence.\n",
    "\n",
    "The need for **context-aware spelling correction** is evident here, as **n-grams** (which capture context) allow for more accurate predictions compared to Norvig’s unigram-based approach.\n",
    "\n",
    "\n",
    "### 2. Choice of Brown NLTK Dataset and 5-grams Dataset\n",
    "\n",
    "The **Brown corpus** is used because it’s a large and diverse dataset, ideal for training n-gram models. The **5-grams** dataset was chosen for its ability to capture longer dependencies between words, which enhances context-aware corrections.\n",
    "\n",
    "The data was divided into **train**, **dev**, and **test** sets to ensure proper evaluation. The **train set** helps build the n-gram model, the **dev set** fine-tunes the model, and the **test set** evaluates its final performance.\n",
    "\n",
    "In the `ContextSensitiveSpellCorrector` class, n-grams are stored in a `defaultdict(int)` for each n-gram size (1-5). This structure efficiently counts occurrences and handles missing keys by initializing them to 0, reducing memory overhead.\n",
    "\n",
    "N-grams are serialized using `pickle`, allowing for fast storage and retrieval. This prevents reprocessing large datasets, making the model faster on subsequent runs. By storing preprocessed n-grams, we save time and resources when working with large datasets like the Brown corpus.\n",
    "\n",
    "\n",
    "### 3. Tokenizer Usage\"\n",
    "\n",
    "The **`TreebankWordTokenizer`** splits contractions like **\"don't\"** into two tokens: **\"do\"** and **\"n't\"**, ensuring accurate n-gram representation. This is crucial for capturing context and avoiding misinterpretations. Without this, space-splitting would incorrectly handle contractions, leading to inaccurate models. \n",
    "\n",
    "### 4. Probability Calculation and Use of Dev Set for Parameter Tuning\n",
    "\n",
    "The probability calculation uses bigram probabilities with **add-k smoothing**\", falling back to unigrams when bigrams are not available. This approach ensures smooth handling of rare or unseen word pairs. The dev set was essential for choosing n = 2 for n-grams and adjusting the alpha parameter, which controls the amount of smoothing applied.\n",
    "\n",
    "### 5. Context-Sensitive Corrector with N-Grams\n",
    "\n",
    "The context-sensitive corrector uses **Maximum Likelihood Estimation (MLE)** to calculate the probability of a word given its context using **logarithms** to avoid underflow.\n",
    "\n",
    "**Beam search** keeps the top \\( k \\) candidate sequences, reducing the computational cost. The beam width, typically tuned to 10 based on dev set performance, limits the number of sequences explored, ensuring efficiency. A **heap** is used to efficiently manage and update the top candidates at each step of beam search, ensuring quick access to the most probable sequences.\n",
    "\n",
    "For the sentence **\"He did not fel well\"**, the beam search explores corrections for \"fel\" like \"feel\", \"fell\", and \"felt\". It calculates the log-probabilities for each candidate and selects the sequence with the highest cumulative log-probability, such as **\"He did not feel well\"**.\n",
    "\n",
    "The beam width of 10, tuned on the dev set, ensures the model efficiently finds the best correction.\n",
    "\n",
    "### 6. Fair evaluation\n",
    "\n",
    "I wanted to test on data close to real-world scenarios, so I used the `sage` library to generate realistic test data. This library provides misspelling statistics based on common errors, ensuring the dataset reflects actual user errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import heapq\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from nltk import TreebankWordTokenizer\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T21:47:20.469279Z",
     "iopub.status.busy": "2025-02-25T21:47:20.468830Z",
     "iopub.status.idle": "2025-02-25T21:47:26.323253Z",
     "shell.execute_reply": "2025-02-25T21:47:26.322254Z",
     "shell.execute_reply.started": "2025-02-25T21:47:20.469242Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/lianamardanova/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lianamardanova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split: 45872 train, 2867 dev test, 8601 test\n",
      "Files saved: brown_train.txt, brown_dev_test.txt, brown_test.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load, shuffle, split (80% train, 5% dev test, 15% test), save\n",
    "def load_data():\n",
    "    nltk.download(\"brown\")\n",
    "    nltk.download('punkt')\n",
    "    sentences = list(brown.sents())\n",
    "    random.shuffle(sentences)  # Ensure random distribution\n",
    "\n",
    "    # Compute split sizes\n",
    "    total = len(sentences)\n",
    "    train_size = int(0.80 * total)\n",
    "    dev_test_size = int(0.05 * total)\n",
    "    test_size = total - train_size - dev_test_size\n",
    "\n",
    "    # Split into datasets\n",
    "    train_sentences = sentences[:train_size]\n",
    "    dev_test_sentences = sentences[train_size : train_size + dev_test_size]\n",
    "    test_sentences = sentences[train_size + dev_test_size :]\n",
    "\n",
    "    # Convert lists of words into sentences\n",
    "    def format_sentences(sent_list):\n",
    "        return [\" \".join(sent) for sent in sent_list]\n",
    "\n",
    "    train_data = format_sentences(train_sentences)\n",
    "    dev_test_data = format_sentences(dev_test_sentences)\n",
    "    test_data = format_sentences(test_sentences)\n",
    "\n",
    "    # Save to files\n",
    "    with open(\"brown_train.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(train_data))\n",
    "    with open(\"brown_dev_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(dev_test_data))\n",
    "    with open(\"brown_test.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(test_data))\n",
    "\n",
    "    print(f\"Data split: {train_size} train, {dev_test_size} dev test, {test_size} test\")\n",
    "    print(\"Files saved: brown_train.txt, brown_dev_test.txt, brown_test.txt\")\n",
    "\n",
    "\n",
    "load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Context Sensitive Spell Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:08.629103Z",
     "iopub.status.busy": "2025-02-25T22:31:08.628689Z",
     "iopub.status.idle": "2025-02-25T22:31:12.161711Z",
     "shell.execute_reply": "2025-02-25T22:31:12.160867Z",
     "shell.execute_reply.started": "2025-02-25T22:31:08.629067Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ContextSensitiveSpellCorrector:\n",
    "    \"\"\"\n",
    "    A spell corrector that uses n-grams (1-5) to improve correction accuracy based on context.\n",
    "    It processes text data to extract n-grams, applies probabilistic models for correction,\n",
    "    and supports both single-word and sentence-level corrections using beam search.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=False):\n",
    "        \"\"\"Initialize and load or process n-grams.\"\"\"\n",
    "        self.pkl_path = \"ngrams.pkl\"\n",
    "        self.ngram_dicts = {\n",
    "            n: defaultdict(int) for n in range(1, 6)\n",
    "        }  # Stores n-grams of different lengths (1-5)\n",
    "        self.verbose = verbose\n",
    "        self.tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "        if os.path.exists(self.pkl_path):\n",
    "            with open(self.pkl_path, \"rb\") as f:\n",
    "                self.ngram_dicts = pickle.load(\n",
    "                    f\n",
    "                )  # Loads precomputed n-grams from cache if available\n",
    "            if self.verbose:\n",
    "                print(\"Loaded n-grams from cache.\")\n",
    "        else:\n",
    "            self.process_ngrams()  # Otherwise, extract and save n-grams from training data\n",
    "\n",
    "        self.unigrams = self.ngram_dicts[1]  # Unigrams dictionary for quick word lookup\n",
    "\n",
    "    def process_ngrams(self):\n",
    "        \"\"\"Extracts n-grams from files and saves them.\"\"\"\n",
    "        self.load_ngrams(\"fivegrams.txt\")  # Load precomputed 5-grams\n",
    "        self.extract_ngrams(\n",
    "            \"brown_train.txt\"\n",
    "        )  # Extract additional n-grams from training text\n",
    "        with open(self.pkl_path, \"wb\") as f:\n",
    "            pickle.dump(\n",
    "                self.ngram_dicts, f\n",
    "            )  # Cache n-grams to avoid reprocessing in future runs\n",
    "        if self.verbose:\n",
    "            print(\"N-grams processed and saved.\")\n",
    "\n",
    "    def load_ngrams(self, file_path):\n",
    "        \"\"\"Loads 5-grams from a file and extracts all n-grams (1-5).\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Loading 5-grams\"):\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) == 6:\n",
    "                count = int(\n",
    "                    parts[0]\n",
    "                )  # First element is the occurrence count of the 5-gram\n",
    "                words = tuple(\n",
    "                    parts[1:]\n",
    "                )  # Remaining elements are the words forming the 5-gram\n",
    "                for n in range(1, 6):\n",
    "                    for i in range(len(words) - n + 1):\n",
    "                        self.ngram_dicts[n][words[i : i + n]] += (\n",
    "                            count  # Populate all n-gram levels\n",
    "                        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"5-grams loaded.\")\n",
    "\n",
    "    def extract_ngrams(self, file_path):\n",
    "        \"\"\"Extracts n-grams (1-5) from a text file and adds to existing dictionary.\"\"\"\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in tqdm(lines, desc=\"Extracting N-Grams\"):\n",
    "            tokens = nltk.word_tokenize(\n",
    "                line.lower()\n",
    "            )  # Tokenizes input text for n-gram extraction\n",
    "            for n in range(1, 6):\n",
    "                if len(tokens) >= n:\n",
    "                    for ngram in nltk.ngrams(tokens, n):\n",
    "                        self.ngram_dicts[n][ngram] += (\n",
    "                            1  # Counts occurrences of extracted n-grams\n",
    "                        )\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"New n-grams extracted.\")\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"\n",
    "        Tokenizes text using NLTK's TreebankWordTokenizer.\n",
    "        \"\"\"\n",
    "        return self.tokenizer.tokenize(text.lower())\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"\n",
    "        Returns the subset of words that appear in the unigrams dictionary.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if (w,) in self.unigrams)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"\"\"\n",
    "        Returns all words that are one edit away from the given word.\n",
    "        \"\"\"\n",
    "        letters = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(\n",
    "            deletes + transposes + replaces + inserts\n",
    "        )  # Generates possible misspellings as in Norwig's solution\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"\n",
    "        Returns all words that are two edits away from the given word.\n",
    "        \"\"\"\n",
    "        return set(\n",
    "            e2 for e1 in self.edits1(word) for e2 in self.edits1(e1)\n",
    "        )  # Expands edit distance\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"\"\"\n",
    "        Generates possible spelling corrections for a word.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.known([word])  # If word exists in unigrams, return it\n",
    "            or self.known(\n",
    "                self.edits1(word)\n",
    "            )  # Otherwise, check words with one edit distance\n",
    "            or self.known(\n",
    "                self.edits2(word)\n",
    "            )  # If still not found, try two edit distances\n",
    "            or {word}  # If all else fails, return original word\n",
    "        )\n",
    "\n",
    "    def probability(self, context, word, alpha=0.5, smoothing=\"add-k\"):\n",
    "        \"\"\"\n",
    "        Computes probability using bigrams and falls back to unigrams if necessary.\n",
    "\n",
    "        Parameters:\n",
    "        - context: Previous words in the sentence (list of strings)\n",
    "        - word: Candidate word to evaluate\n",
    "        - alpha: Smoothing parameter (used in Laplace smoothing)\n",
    "        - smoothing: Smoothing technique ('add-k')\n",
    "\n",
    "        Returns:\n",
    "        - Probability score (float)\n",
    "        \"\"\"\n",
    "        total_unigrams = sum(self.ngram_dicts[1].values())\n",
    "\n",
    "        if len(context) >= 1:\n",
    "            bigram = tuple(context[-1:] + [word])\n",
    "            bigram_count = self.ngram_dicts[2].get(\n",
    "                bigram, 0\n",
    "            )  # Get frequency of bigram occurrence\n",
    "            unigram_count = self.ngram_dicts[1].get(\n",
    "                (context[-1],), 0\n",
    "            )  # Get frequency of preceding word\n",
    "\n",
    "            if smoothing == \"add-k\":\n",
    "                return (bigram_count + alpha) / (\n",
    "                    unigram_count\n",
    "                    + alpha * len(self.ngram_dicts[1])  # Apply Laplace smoothing\n",
    "                )\n",
    "\n",
    "        unigram_prob = (self.ngram_dicts[1].get((word,), 0) + alpha) / (\n",
    "            total_unigrams\n",
    "            + alpha * len(self.ngram_dicts[1])  # Fallback to unigram probability\n",
    "        )\n",
    "        return unigram_prob\n",
    "\n",
    "    def correct_word(self, word, context=None):\n",
    "        \"\"\"\n",
    "        Greedy correction for a single word.\n",
    "        \"\"\"\n",
    "        candidates = self.candidates(word)  # Generate candidate corrections\n",
    "        if not context:\n",
    "            return max(\n",
    "                candidates, key=lambda w: self.ngram_dicts[1].get((w,), 0)\n",
    "            )  # Select based on unigram frequency\n",
    "        else:\n",
    "            return max(\n",
    "                candidates, key=lambda w: self.probability(context, w)\n",
    "            )  # Select based on context probability\n",
    "\n",
    "    def correct_sentence(self, sentence, beam_width=10):\n",
    "        \"\"\"\n",
    "        Corrects a sentence using beam search.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(sentence)\n",
    "        beam = [(0.0, [])]  # Initialize beam search with empty sequence\n",
    "\n",
    "        for i, word in enumerate(tokens):\n",
    "            if self.verbose:\n",
    "                print(f\"\\nProcessing token {i + 1}/{len(tokens)}: '{word}'\")\n",
    "            new_beam = []\n",
    "\n",
    "            for score, seq in beam:\n",
    "                context = seq[-4:]  # Use up to 4 previous words as context\n",
    "                candidate_set = self.candidates(word)\n",
    "\n",
    "                for cand in candidate_set:\n",
    "                    new_seq = seq + [cand]\n",
    "                    new_score = 0.0\n",
    "\n",
    "                    for j in range(len(new_seq)):\n",
    "                        sub_context = new_seq[max(0, j - 4) : j]\n",
    "                        current_word = new_seq[j]\n",
    "                        prob = max(\n",
    "                            1e-10, self.probability(sub_context, current_word)\n",
    "                        )  # Avoid log(0)\n",
    "                        new_score += math.log(prob)\n",
    "\n",
    "                    heapq.heappush(new_beam, (new_score, new_seq))\n",
    "                    if len(new_beam) > beam_width:\n",
    "                        heapq.heappop(new_beam)  # Keep only the top candidates\n",
    "\n",
    "                    if self.verbose:\n",
    "                        print(\n",
    "                            f\"Candidate: '{cand}', Context: {context}, New Seq: {new_seq}, Score: {new_score}\"\n",
    "                        )\n",
    "\n",
    "            beam = new_beam\n",
    "\n",
    "        _, best_seq = max(beam, key=lambda x: x[0])  # Select best corrected sentence\n",
    "        return \" \".join(best_seq)\n",
    "\n",
    "\n",
    "context_sensetive_corrector = ContextSensitiveSpellCorrector(verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1-grams:\n",
      "  ('a',): 2799082\n",
      "  ('babe',): 34\n",
      "  ('in',): 1911331\n",
      "  ('the',): 7425956\n",
      "  ('woods',): 1895\n",
      "  ('baby',): 4120\n",
      "  ('at',): 585266\n",
      "  ('her',): 217483\n",
      "  ('breast',): 2448\n",
      "  ('brother',): 1728\n",
      "\n",
      "Top 2-grams:\n",
      "  ('a', 'babe'): 16\n",
      "  ('babe', 'in'): 16\n",
      "  ('in', 'the'): 852298\n",
      "  ('the', 'woods'): 1760\n",
      "  ('a', 'baby'): 1740\n",
      "  ('baby', 'at'): 14\n",
      "  ('at', 'her'): 7574\n",
      "  ('her', 'breast'): 56\n",
      "  ('baby', 'brother'): 9\n",
      "  ('brother', 'or'): 63\n",
      "\n",
      "Top 3-grams:\n",
      "  ('a', 'babe', 'in'): 16\n",
      "  ('babe', 'in', 'the'): 16\n",
      "  ('in', 'the', 'woods'): 636\n",
      "  ('a', 'baby', 'at'): 14\n",
      "  ('baby', 'at', 'her'): 6\n",
      "  ('at', 'her', 'breast'): 6\n",
      "  ('a', 'baby', 'brother'): 9\n",
      "  ('baby', 'brother', 'or'): 9\n",
      "  ('brother', 'or', 'sister'): 41\n",
      "  ('a', 'baby', 'crying'): 16\n",
      "\n",
      "Top 4-grams:\n",
      "  ('a', 'babe', 'in', 'the'): 16\n",
      "  ('babe', 'in', 'the', 'woods'): 16\n",
      "  ('a', 'baby', 'at', 'her'): 6\n",
      "  ('baby', 'at', 'her', 'breast'): 6\n",
      "  ('a', 'baby', 'brother', 'or'): 9\n",
      "  ('baby', 'brother', 'or', 'sister'): 9\n",
      "  ('a', 'baby', 'crying', 'in'): 6\n",
      "  ('baby', 'crying', 'in', 'the'): 6\n",
      "  ('a', 'baby', 'girl', 'was'): 6\n",
      "  ('baby', 'girl', 'was', 'born'): 6\n",
      "\n",
      "Top 5-grams:\n",
      "  ('a', 'babe', 'in', 'the', 'woods'): 16\n",
      "  ('a', 'baby', 'at', 'her', 'breast'): 6\n",
      "  ('a', 'baby', 'brother', 'or', 'sister'): 9\n",
      "  ('a', 'baby', 'crying', 'in', 'the'): 6\n",
      "  ('a', 'baby', 'girl', 'was', 'born'): 6\n",
      "  ('a', 'baby', 'in', 'a', 'stroller'): 8\n",
      "  ('a', 'baby', 'in', 'her', 'arms'): 28\n",
      "  ('a', 'baby', 'in', 'his', 'arms'): 7\n",
      "  ('a', 'baby', 'in', 'the', 'house'): 11\n",
      "  ('a', 'baby', 'into', 'the', 'world'): 12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top n-grams for each n (1-5)\n",
    "top_ngrams = {}\n",
    "for n in range(1, 6):\n",
    "    top_ngrams[n] = list(context_sensetive_corrector.ngram_dicts[n].items())[:10]\n",
    "\n",
    "# Display the top 5n-grams for each length\n",
    "for n in range(1, 6):\n",
    "    print(f\"Top {n}-grams:\")\n",
    "    for ngram, count in top_ngrams[n]:\n",
    "        print(f\"  {ngram}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAJOCAYAAAAqFJGJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABFa0lEQVR4nO3deVxWZf7/8fetCLiB4QI4Ii6ZilsmKmha5hYqo6Vpi7ikNqalyTgZqZM4jkup4W6WRTa5VOaWlmKmWGqJgS2TjjaaZjeRGwiNmHJ+f/Tj/nrHInCht8jr+Xicx8Nz3de5zueiU/H2OufcNsuyLAEAAACAgTKuLgAAAABAyUewAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAHBLiY2Nlc1mk6enp3744Yccn997771q2rRpocb8+eef9fzzz+vOO++Ul5eX3N3dVatWLT344IPauHGjrly5Ulzl3xCbNm1SeHi4fH195e7uLh8fH3Xu3Flvv/22fvvtN1eXJ0maPn261q9fX+zjrlixQtWrV9eFCxccbR988IEGDRqkZs2aqVy5crLZbIUe12az5brNnDmzwGMcO3ZMY8aMUePGjVWxYkV5enqqTp06GjhwoD755BNZllXoulzt448/VqVKlXTq1ClXlwLgBiBYALglZWZmatKkScbj7Nu3T82aNdOrr76qP//5z1q9erW2b9+umTNnqly5cnrwwQcVGxtrXvANYFmWhg4dqj//+c/KysrS3LlztX37dr355ptq0aKFRo0apcWLF7u6TEnXJ1j8+uuvev755zVhwgRVrlzZ0b5u3Trt27dPQUFBatGiRZHH79evn/bu3eu0DRo0qEDHbty4Uc2aNdPGjRs1ePBgrVu3Tlu3btXkyZN15swZ3XfffdqxY0eRa3OVzp07q02bNnr++eddXQqAG8ECgFvIG2+8YUmy7r//fqtMmTJWUlKS0+f33HOP1aRJkwKNde7cOcvX19eqW7eu9dNPP+Xa5+DBg9aOHTvyHefXX3+1srKyCjaB62jWrFmWJCs6OjrXz+12u7V79+4bXFXuKlasaA0ePLhYx1y8eLHl6elpnTt3zqn9ypUrjj+PHj3aKsr/GiVZo0ePLlJdR48etSpUqGC1bt3aSk1NzbXPJ598kuNa/qOMjIwinf96e++996yyZctaJ06ccHUpAK4zViwA3JKeffZZVa1aVRMmTCjyGK+++qp+/vlnvfjii/L398+1T/PmzdWpUyfHfvatWNu2bdPjjz+u6tWrq0KFCsrMzNTRo0c1dOhQNWjQQBUqVNCf/vQnhYeH6+uvv3Yac+fOnbLZbFq5cqUmTJggf39/VapUSeHh4fr555914cIFPfHEE6pWrZqqVaumoUOHKj09Pd+5/Pbbb5o1a5YaNWqkyZMn59rHz89Pd999t2P/7NmzGjVqlP70pz/J3d1d9erV08SJE5WZmenoc/z4cdlstlxXbWw2m6ZMmeLYnzJlimw2m7799ls98sgj8vb2lq+vrx5//HGlpqY6HZeRkaE333zTcUvRvffeK+n3VYfx48erbt268vT0lI+Pj4KDg7Vq1ap85y9JS5YsUXh4uKpUqeLUXqaMa/9XOHfuXP36669avHixvLy8cu1z7733Oq2mZP8sv/zyS/Xr10+33Xab6tevL0lKSEjQww8/rDp16qh8+fKqU6eOHnnkkRy3BmZfqzt27NCIESNUtWpVeXl5adCgQcrIyFBycrL69++vKlWqyN/fX+PHj89xq9ySJUvUokULVapUSZUrV1ajRo1yrE6Eh4erUqVKevXVV4vjxwXgJkawAHBLqly5siZNmqStW7cW+RaSuLg4lS1bVj169Cj0sY8//rjKlSunt956S++9957KlSunn376SVWrVtXMmTP10UcfadGiRXJzc1Pbtm11+PDhHGM8//zzSklJUWxsrObMmaOdO3fqkUceUd++feXt7a1Vq1bp2Wef1VtvvXXNW00SEhJ09uxZ9e7du0DPEFy8eFGdOnXSihUrFBkZqc2bN2vgwIF68cUX9eCDDxb653G1vn376o477tDatWv13HPPaeXKlRo3bpzj871796p8+fLq0aOH45ai7Fu0IiMjtWTJEo0ZM0YfffSR3nrrLT300EM6c+ZMvuf88ccf9fXXXzuFwOK2cuVKlS9fXh4eHmrVqpXeeOONAh0XFxcnf39/BQcHF/qcDz74oG6//Xa9++67Wrp0qaTfw17Dhg0VExOjrVu3atasWbLb7WrdurVOnz6dY4zhw4fL29tbq1ev1qRJk7Ry5UqNGDFCPXv2VIsWLfTee+9p8ODBmjNnjhYsWOA4bvXq1Ro1apTuuecerVu3TuvXr9e4ceOUkZHhNL67u7vatWunzZs3F3p+AEoYVy+ZAEBxyr4Vav/+/VZmZqZVr149Kzg42HErUmFuhWrUqJHl5+eXo/3KlSvWb7/95tiuvpUm+/yDBg265viXL1+2Ll26ZDVo0MAaN26co/2TTz6xJFnh4eFO/Z955hlLkjVmzBin9j59+lg+Pj75nmv16tWWJGvp0qXXrMuyLGvp0qWWJOudd95xas++nWrbtm2WZVnWsWPHLEnWG2+8kWMMSdYLL7zg2H/hhRcsSdaLL77o1G/UqFGWp6en0+1ied0K1bRpU6tPnz4FmsPV1qxZY0my9u3bl2+/ot4K9eijj1pvv/22FR8fb7333ntWWFiYJcmaNGnSNY/19PS0QkJCcrTnd51l/yz//ve/X3P8y5cvW+np6VbFihWtefPmOdqzr9Wnn37aqX+fPn0sSdbcuXOd2u+8807rrrvucuw/9dRTVpUqVa55fsuyrIkTJ1plypSx0tPTC9QfQMnEisU1xMfHKzw8XDVr1pTNZivSw4SWZWn27Nm644475OHhoYCAAE2fPr34iwXgxN3dXdOmTVNCQoLeeeedXPtkZWXp8uXLjq0gb3iKjIxUuXLlHNuf//znHH369u2bo+3y5cuaPn26goKC5O7uLjc3N7m7u+vIkSP67rvvcvTv1auX037jxo0lST179szRfvbs2WveDlUYO3bsUMWKFdWvXz+n9iFDhkj6/W0/RfXHn1fz5s118eJFpaSkXPPYNm3a6MMPP9Rzzz2nnTt36n//+1+BzvnTTz9JkmrUqFH4gv+/q6+Ty5cvO72l6e2339ajjz6qDh06qG/fvtqyZYt69eqlmTNn6pdffinS+R588EGn62zMmDE5+uR2naWnp2vChAm6/fbb5ebmJjc3N1WqVEkZGRnG19nVt1O1adNG58+f1yOPPKINGzbkuhqSrUaNGsrKylJycnL+kwZQohEsriEjI0MtWrTQwoULizzG2LFj9dprr2n27Nk6dOiQNm3apDZt2hRjlQDy8vDDD+uuu+7SxIkTc32VavYtS9lb586dHZ/Vrl1bv/zyi3799VenY/76179q//792r9/f57PXuTWHhkZqcmTJ6tPnz7atGmTPv/8c+3fv18tWrTI9RdkHx8fp313d/d82y9evJhrLdlzkX5/pWlBnDlzRn5+fjlum6pRo4bc3NyueetRfqpWreq07+HhIUkFCgnz58/XhAkTtH79enXq1Ek+Pj7q06ePjhw5ku9x2WN7enoWsWo5XSflypXTm2++mW//gQMH6vLly0pISMi3X+3atXN9NfKcOXMc11lecrvOHn30US1cuFDDhw/X1q1b9cUXX2j//v2qXr268XV29TUWERGh119/XT/88IP69u2rGjVqqG3btoqLi8txjuyfe0GDIICSyc3VBdzswsLCFBYWlufnly5d0qRJk/T222/r/Pnzatq0qWbNmuV40PC7777TkiVL9M0336hhw4Y3qGoA2Ww2m2bNmqWuXbtq2bJlOT6fMmWKnnrqKcf+1a8h7dq1q7Zt26YtW7Y4/c19QECAAgICJP3fL2G5nfeP/vWvf2nQoEE5VixPnz6d44Hi4hYcHCwfHx9t2LBBM2bMuOZzFlWrVtXnn38uy7Kc+qakpOjy5cuqVq2apP/7hfHqB7olGQWP/FSsWFHR0dGKjo7Wzz//7Fi9CA8P16FDh/I8Lrves2fP5hkGr+WPv+DXrVs33/7ZKxrXeji8a9euWrRokRISEpyes8h+GDs/f/znmJqaqg8++EAvvPCCnnvuOUd7Zmamzp49e83xCmvo0KEaOnSoMjIyFB8frxdeeEG9evXSf/7zHwUGBjr6ZZ87+58DgFsTKxaGhg4dqs8++0yrV6/WV199pYceekj333+/42/PNm3apHr16umDDz5Q3bp1VadOHQ0fPvy6/AceQO66dOmirl27aurUqTluF6pTp46Cg4Md29V/ATB8+HD5+vrq2Wefld1uN67DZrM5/nY+2+bNm2/Il4eVK1dOEyZM0KFDh/SPf/wj1z4pKSn67LPPJP3+/QPp6ek5bv9csWKF43NJ8vX1laenp7766iunfhs2bDCq18PD45p/u+3r66shQ4bokUce0eHDh3OsLF2tUaNGkqTvv/++yDVdfZ0EBwfnWHn5o7feekvlypVTq1at8u03btw4VahQQaNHj3b64r6isNlssiwrx3X22muvXdcvcqxYsaLCwsI0ceJEXbp0Sd9++63T5//9739VtWpV+fr6XrcaALgeKxYGvv/+e61atUo//vijatasKUkaP368PvroI73xxhuaPn26/vvf/+qHH37Qu+++qxUrVujKlSsaN26c+vXrVyK/7AgoqWbNmqVWrVopJSVFTZo0KdAxVapU0fr16xUeHq4WLVroySefVEhIiCpVqqQzZ84oPj5eycnJateuXYHG69Wrl2JjY9WoUSM1b95cBw4c0EsvvaRatWqZTK3A/va3v+m7777TCy+8oC+++EKPPvqoAgIClJqaqvj4eC1btkzR0dFq3769Bg0apEWLFmnw4ME6fvy4mjVrpk8//VTTp09Xjx491KVLF0m//yI7cOBAvf7666pfv75atGihL774QitXrjSqtVmzZtq5c6c2bdokf39/Va5cWQ0bNlTbtm3Vq1cvNW/eXLfddpu+++47vfXWWwoNDVWFChXyHK9t27YqX7689u3bl+MZjx9++MGxGpEdPN577z1J/xc88/PSSy/p3//+tzp37qxatWopJSVFy5cv17Zt2zRlypRr/i19/fr1tWrVKj3yyCNq1qyZnnzySd11113y8PBQSkqKtm3bJkl5vor2al5eXurYsaNeeuklVatWTXXq1NGuXbu0fPnyYl8VGzFihMqXL6/27dvL399fycnJmjFjhry9vdW6dWunvvv27dM999xTpG81B1CCuPTR8RJGkrVu3TrH/jvvvGNJsipWrOi0ubm5Wf3797csy7JGjBhhSbIOHz7sOO7AgQOWJOvQoUM3egrALe/qt0L90aOPPmpJKvBbobIlJydbUVFRVvPmza2KFSta5cqVs2rWrGmFh4dbK1assH777bcCnf/cuXPWsGHDrBo1algVKlSw7r77bmv37t3WPffcY91zzz2OftlvhXr33XcLNLfsNwT98ssvBZrPhg0brJ49e1rVq1e33NzcrNtuu83q1KmTtXTpUiszM9PR78yZM9bIkSMtf39/y83NzQoMDLSioqKsixcvOo2XmppqDR8+3PL19bUqVqxohYeHW8ePH8/zrVB/rDN7XseOHXO0JSUlWe3bt7cqVKhgSXL8fJ577jkrODjYuu222ywPDw+rXr161rhx46zTp09fc94RERFWUFBQjvbs8+e2FeRL+jZu3Gjdfffdjp9n5cqVrQ4dOlirVq265rFX+/77762nn37aatiwoVW+fHnLw8PDCgwMtB566CFr3bp1Tm/Nyu+f+Y8//mj17dvXuu2226zKlStb999/v/XNN99YgYGBTvMp7PU0ePBgq2LFio79N9980+rUqZPl6+trubu7WzVr1rT69+9vffXVV07HHT161JJkrV27tlA/DwAlj82yrnqtBfJls9m0bt069enTR5K0Zs0aPfbYY/r2229VtmxZp76VKlWSn5+fXnjhBU2fPt3podH//e9/qlChgrZt26auXbveyCkAQKmVkJCg1q1ba9++fWrbtq2ryyk1Jk+erBUrVuj777+Xmxs3SgC3Mv4NN9CyZUtduXJFKSkp6tChQ6592rdvr8uXL+v77793PIj3n//8R5KcHmwDAFxfwcHB6t+/v/7xj3/ogw8+cHU5pcL58+e1aNEiLViwgFABlAKsWFxDenq6jh49Kun3IDF37lzHKw5r166tgQMH6rPPPtOcOXPUsmVLnT59Wjt27FCzZs3Uo0cPZWVlqXXr1qpUqZJiYmKUlZWl0aNHy8vLy3HfLADgxvjxxx+1fPlyRUZGOr0BDNdHYmKitm/frvHjx/N8BVAKECyuYefOnerUqVOO9sGDBys2Nla//fabpk2bphUrVujUqVOqWrWqQkNDFR0drWbNmkn6/YuZnn76aW3bts3x5ow5c+bkeEc4AAAAUFIRLAAAAAAY43ssAAAAABgjWAAAAAAwxisacpGVlaWffvpJlStX5mEzAAAAlFqWZenChQuqWbOmypTJf02CYJGLn376SQEBAa4uAwAAALgpnDx5UrVq1cq3D8EiF9mvIDx58qS8vLxcXA0AAADgGmlpaQoICCjQK7oJFrnIvv3Jy8uLYAEAAIBSryCPB/DwNgAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMObm6gIAAABwY9V5brOrS0AhHJ/Z09UlFAgrFgAAAACMESwAAAAAGHNpsIiPj1d4eLhq1qwpm82m9evX59t/yJAhstlsObYmTZo4+sTGxuba5+LFi9d5NgAAAEDp5dJgkZGRoRYtWmjhwoUF6j9v3jzZ7XbHdvLkSfn4+Oihhx5y6ufl5eXUz263y9PT83pMAQAAAIBc/PB2WFiYwsLCCtzf29tb3t7ejv3169fr3LlzGjp0qFM/m80mPz+/YqsTAAAAQP5K9DMWy5cvV5cuXRQYGOjUnp6ersDAQNWqVUu9evVSYmJivuNkZmYqLS3NaQMAAABQcCU2WNjtdn344YcaPny4U3ujRo0UGxurjRs3atWqVfL09FT79u115MiRPMeaMWOGYzXE29tbAQEB17t8AAAA4JZSYoNFbGysqlSpoj59+ji1h4SEaODAgWrRooU6dOigd955R3fccYcWLFiQ51hRUVFKTU11bCdPnrzO1QMAAAC3lhL5BXmWZen1119XRESE3N3d8+1bpkwZtW7dOt8VCw8PD3l4eBR3mQAAAECpUSJXLHbt2qWjR49q2LBh1+xrWZaSkpLk7+9/AyoDAAAASieXrlikp6fr6NGjjv1jx44pKSlJPj4+ql27tqKionTq1CmtWLHC6bjly5erbdu2atq0aY4xo6OjFRISogYNGigtLU3z589XUlKSFi1adN3nAwAAAJRWLg0WCQkJ6tSpk2M/MjJSkjR48GDFxsbKbrfrxIkTTsekpqZq7dq1mjdvXq5jnj9/Xk888YSSk5Pl7e2tli1bKj4+Xm3atLl+EwEAAABKOZtlWZari7jZpKWlydvbW6mpqfLy8nJ1OQAAAMWqznObXV0CCuH4zJ4uO3dhfi8ukc9YAAAAALi5ECwAAAAAGCNYAAAAADBWIr/HAgAAFAz30pccrryPHigOrFgAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjLg0W8fHxCg8PV82aNWWz2bR+/fp8++/cuVM2my3HdujQIad+a9euVVBQkDw8PBQUFKR169Zdx1kAAAAAcGmwyMjIUIsWLbRw4cJCHXf48GHZ7XbH1qBBA8dne/fu1YABAxQREaGDBw8qIiJC/fv31+eff17c5QMAAAD4/9xcefKwsDCFhYUV+rgaNWqoSpUquX4WExOjrl27KioqSpIUFRWlXbt2KSYmRqtWrTIpFwAAAEAeSuQzFi1btpS/v786d+6sTz75xOmzvXv3qlu3bk5t3bt31549e/IcLzMzU2lpaU4bAAAAgIIrUcHC399fy5Yt09q1a/X++++rYcOG6ty5s+Lj4x19kpOT5evr63Scr6+vkpOT8xx3xowZ8vb2dmwBAQHXbQ4AAADArcilt0IVVsOGDdWwYUPHfmhoqE6ePKnZs2erY8eOjnabzeZ0nGVZOdquFhUVpcjISMd+Wloa4QIAAAAohBK1YpGbkJAQHTlyxLHv5+eXY3UiJSUlxyrG1Tw8POTl5eW0AQAAACi4Eh8sEhMT5e/v79gPDQ1VXFycU59t27apXbt2N7o0AAAAoNRw6a1Q6enpOnr0qGP/2LFjSkpKko+Pj2rXrq2oqCidOnVKK1askPT7G5/q1KmjJk2a6NKlS/rXv/6ltWvXau3atY4xxo4dq44dO2rWrFnq3bu3NmzYoO3bt+vTTz+94fMDAAAASguXBouEhAR16tTJsZ/9nMPgwYMVGxsru92uEydOOD6/dOmSxo8fr1OnTql8+fJq0qSJNm/erB49ejj6tGvXTqtXr9akSZM0efJk1a9fX2vWrFHbtm1v3MQAAACAUsZmWZbl6iJuNmlpafL29lZqairPWwAASrQ6z212dQkooOMze96wc3FdlCw38tr4o8L8Xlzin7EAAAAA4HoECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMZcGi/j4eIWHh6tmzZqy2Wxav359vv3ff/99de3aVdWrV5eXl5dCQ0O1detWpz6xsbGy2Ww5tosXL17HmQAAAAClm0uDRUZGhlq0aKGFCxcWqH98fLy6du2qLVu26MCBA+rUqZPCw8OVmJjo1M/Ly0t2u91p8/T0vB5TAAAAACDJzZUnDwsLU1hYWIH7x8TEOO1Pnz5dGzZs0KZNm9SyZUtHu81mk5+fX3GVCQAAAOAaSvQzFllZWbpw4YJ8fHyc2tPT0xUYGKhatWqpV69eOVY0AAAAABSvEh0s5syZo4yMDPXv39/R1qhRI8XGxmrjxo1atWqVPD091b59ex05ciTPcTIzM5WWlua0AQAAACg4l94KZWLVqlWaMmWKNmzYoBo1ajjaQ0JCFBIS4thv37697rrrLi1YsEDz58/PdawZM2YoOjr6utcMAAAA3KpK5IrFmjVrNGzYML3zzjvq0qVLvn3LlCmj1q1b57tiERUVpdTUVMd28uTJ4i4ZAAAAuKWVuBWLVatW6fHHH9eqVavUs2fPa/a3LEtJSUlq1qxZnn08PDzk4eFRnGUCAAAApYpLg0V6erqOHj3q2D927JiSkpLk4+Oj2rVrKyoqSqdOndKKFSsk/R4qBg0apHnz5ikkJETJycmSpPLly8vb21uSFB0drZCQEDVo0EBpaWmaP3++kpKStGjRohs/QQAAAKCUcOmtUAkJCWrZsqXjVbGRkZFq2bKl/v73v0uS7Ha7Tpw44ej/yiuv6PLlyxo9erT8/f0d29ixYx19zp8/ryeeeEKNGzdWt27ddOrUKcXHx6tNmzY3dnIAAABAKWKzLMtydRE3m7S0NHl7eys1NVVeXl6uLgcAgCKr89xmV5eAAjo+89q3eBcXrouS5UZeG39UmN+LS+TD2wAAAABuLgQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgrMR98zYAICdeHVmyuPLVkQBwvbBiAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMZcGi/j4eIWHh6tmzZqy2Wxav379NY/ZtWuXWrVqJU9PT9WrV09Lly7N0Wft2rUKCgqSh4eHgoKCtG7duutQPQAAAIBsLg0WGRkZatGihRYuXFig/seOHVOPHj3UoUMHJSYm6vnnn9eYMWO0du1aR5+9e/dqwIABioiI0MGDBxUREaH+/fvr888/v17TAAAAAEo9N1eePCwsTGFhYQXuv3TpUtWuXVsxMTGSpMaNGyshIUGzZ89W3759JUkxMTHq2rWroqKiJElRUVHatWuXYmJitGrVqmKfAwAAAIAS9ozF3r171a1bN6e27t27KyEhQb/99lu+ffbs2XPD6gQAAABKG5euWBRWcnKyfH19ndp8fX11+fJlnT59Wv7+/nn2SU5OznPczMxMZWZmOvbT0tKKt3AAAADgFlekFYt69erpzJkzOdrPnz+vevXqGReVH5vN5rRvWVaO9tz6/LHtajNmzJC3t7djCwgIKMaKAQAAgFtfkYLF8ePHdeXKlRztmZmZOnXqlHFRefHz88ux8pCSkiI3NzdVrVo13z5/XMW4WlRUlFJTUx3byZMni794AAAA4BZWqFuhNm7c6Pjz1q1b5e3t7di/cuWKPv74Y9WpU6fYivuj0NBQbdq0yalt27ZtCg4OVrly5Rx94uLiNG7cOKc+7dq1y3NcDw8PeXh4XJ+iAQAAgFKgUMGiT58+kn6/1Wjw4MFOn5UrV0516tTRnDlzCjxeenq6jh496tg/duyYkpKS5OPjo9q1aysqKkqnTp3SihUrJEkjR47UwoULFRkZqREjRmjv3r1avny509uexo4dq44dO2rWrFnq3bu3NmzYoO3bt+vTTz8tzFQBAAAAFEKhgkVWVpYkqW7dutq/f7+qVatmdPKEhAR16tTJsR8ZGSlJGjx4sGJjY2W323XixAnH53Xr1tWWLVs0btw4LVq0SDVr1tT8+fMdr5qVpHbt2mn16tWaNGmSJk+erPr162vNmjVq27atUa0AAAAA8lakt0IdO3asWE5+7733Oh6+zk1sbGyOtnvuuUdffvllvuP269dP/fr1My0PAAAAQAEV+XWzH3/8sT7++GOlpKQ4VjKyvf7668aFAQAAACg5ihQsoqOjNXXqVAUHB8vf3z/fV7kCAAAAuPUVKVgsXbpUsbGxioiIKO56AAAAAJRARfoei0uXLuX7+lYAAAAApUuRgsXw4cO1cuXK4q4FAAAAQAlVpFuhLl68qGXLlmn79u1q3ry548vpss2dO7dYigMAAABQMhQpWHz11Ve68847JUnffPON02c8yA0AAACUPkUKFp988klx1wEAAACgBCvSMxYAAAAAcLUirVh06tQp31ueduzYUeSCAAAAAJQ8RQoW2c9XZPvtt9+UlJSkb775RoMHDy6OugAAAACUIEUKFi+//HKu7VOmTFF6erpRQQAAAABKnmJ9xmLgwIF6/fXXi3NIAAAAACVAsQaLvXv3ytPTsziHBAAAAFACFOlWqAcffNBp37Is2e12JSQkaPLkycVSGAAAAICSo0jBwtvb22m/TJkyatiwoaZOnapu3boVS2EAAAAASo4iBYs33nijuOsAAAAAUIIVKVhkO3DggL777jvZbDYFBQWpZcuWxVUXAAAAgBKkSMEiJSVFDz/8sHbu3KkqVarIsiylpqaqU6dOWr16tapXr17cdQIAAAC4iRXprVBPP/200tLS9O233+rs2bM6d+6cvvnmG6WlpWnMmDHFXSMAAACAm1yRViw++ugjbd++XY0bN3a0BQUFadGiRTy8DQAAAJRCRVqxyMrKUrly5XK0lytXTllZWcZFAQAAAChZihQs7rvvPo0dO1Y//fSTo+3UqVMaN26cOnfuXGzFAQAAACgZihQsFi5cqAsXLqhOnTqqX7++br/9dtWtW1cXLlzQggULirtGAAAAADe5Ij1jERAQoC+//FJxcXE6dOiQLMtSUFCQunTpUtz1AQAAACgBCrVisWPHDgUFBSktLU2S1LVrVz399NMaM2aMWrdurSZNmmj37t3XpVAAAAAAN69CBYuYmBiNGDFCXl5eOT7z9vbWX/7yF82dO7fYigMAAABQMhQqWBw8eFD3339/np9369ZNBw4cMC4KAAAAQMlSqGDx888/5/qa2Wxubm765ZdfjIsCAAAAULIUKlj86U9/0tdff53n51999ZX8/f2NiwIAAABQshQqWPTo0UN///vfdfHixRyf/e9//9MLL7ygXr16FVtxAAAAAEqGQr1udtKkSXr//fd1xx136KmnnlLDhg1ls9n03XffadGiRbpy5YomTpx4vWoFAAAAcJMqVLDw9fXVnj179OSTTyoqKkqWZUmSbDabunfvrsWLF8vX1/e6FAoAAADg5lXoL8gLDAzUli1bdO7cOR09elSWZalBgwa67bbbrkd9AAAAAEqAIn3ztiTddtttat26dXHWAgAAAKCEKtTD2wAAAACQG4IFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMYIFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAw5vJgsXjxYtWtW1eenp5q1aqVdu/enWffIUOGyGaz5diaNGni6BMbG5trn4sXL96I6QAAAAClkkuDxZo1a/TMM89o4sSJSkxMVIcOHRQWFqYTJ07k2n/evHmy2+2O7eTJk/Lx8dFDDz3k1M/Ly8upn91ul6en542YEgAAAFAquTRYzJ07V8OGDdPw4cPVuHFjxcTEKCAgQEuWLMm1v7e3t/z8/BxbQkKCzp07p6FDhzr1s9lsTv38/PxuxHQAAACAUstlweLSpUs6cOCAunXr5tTerVs37dmzp0BjLF++XF26dFFgYKBTe3p6ugIDA1WrVi316tVLiYmJxVY3AAAAgJzcXHXi06dP68qVK/L19XVq9/X1VXJy8jWPt9vt+vDDD7Vy5Uqn9kaNGik2NlbNmjVTWlqa5s2bp/bt2+vgwYNq0KBBrmNlZmYqMzPTsZ+WllaEGQEAAACll8sf3rbZbE77lmXlaMtNbGysqlSpoj59+ji1h4SEaODAgWrRooU6dOigd955R3fccYcWLFiQ51gzZsyQt7e3YwsICCjSXAAAAIDSymXBolq1aipbtmyO1YmUlJQcqxh/ZFmWXn/9dUVERMjd3T3fvmXKlFHr1q115MiRPPtERUUpNTXVsZ08ebLgEwEAAADgumDh7u6uVq1aKS4uzqk9Li5O7dq1y/fYXbt26ejRoxo2bNg1z2NZlpKSkuTv759nHw8PD3l5eTltAAAAAArOZc9YSFJkZKQiIiIUHBys0NBQLVu2TCdOnNDIkSMl/b6ScOrUKa1YscLpuOXLl6tt27Zq2rRpjjGjo6MVEhKiBg0aKC0tTfPnz1dSUpIWLVp0Q+YEAAAAlEYuDRYDBgzQmTNnNHXqVNntdjVt2lRbtmxxvOXJbrfn+E6L1NRUrV27VvPmzct1zPPnz+uJJ55QcnKyvL291bJlS8XHx6tNmzbXfT4AAABAaeXSYCFJo0aN0qhRo3L9LDY2Nkebt7e3fv311zzHe/nll/Xyyy8XV3kAAAAACsDlb4UCAAAAUPIRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIy5PFgsXrxYdevWlaenp1q1aqXdu3fn2Xfnzp2y2Ww5tkOHDjn1W7t2rYKCguTh4aGgoCCtW7fuek8DAAAAKNVcGizWrFmjZ555RhMnTlRiYqI6dOigsLAwnThxIt/jDh8+LLvd7tgaNGjg+Gzv3r0aMGCAIiIidPDgQUVERKh///76/PPPr/d0AAAAgFLLpcFi7ty5GjZsmIYPH67GjRsrJiZGAQEBWrJkSb7H1ahRQ35+fo6tbNmyjs9iYmLUtWtXRUVFqVGjRoqKilLnzp0VExNznWcDAAAAlF4uCxaXLl3SgQMH1K1bN6f2bt26ac+ePfke27JlS/n7+6tz58765JNPnD7bu3dvjjG7d+9+zTEBAAAAFJ2bq058+vRpXblyRb6+vk7tvr6+Sk5OzvUYf39/LVu2TK1atVJmZqbeeustde7cWTt37lTHjh0lScnJyYUaU5IyMzOVmZnp2E9LSyvqtAAAAIBSyWXBIpvNZnPatywrR1u2hg0bqmHDho790NBQnTx5UrNnz3YEi8KOKUkzZsxQdHR0UcoHbrg6z212dQkooOMze7q6BAAAbhiX3QpVrVo1lS1bNsdKQkpKSo4Vh/yEhIToyJEjjn0/P79CjxkVFaXU1FTHdvLkyQKfHwAAAIALg4W7u7tatWqluLg4p/a4uDi1a9euwOMkJibK39/fsR8aGppjzG3btuU7poeHh7y8vJw2AAAAAAXn0luhIiMjFRERoeDgYIWGhmrZsmU6ceKERo4cKen3lYRTp05pxYoVkn5/41OdOnXUpEkTXbp0Sf/617+0du1arV271jHm2LFj1bFjR82aNUu9e/fWhg0btH37dn366acumSMAAABQGrg0WAwYMEBnzpzR1KlTZbfb1bRpU23ZskWBgYGSJLvd7vSdFpcuXdL48eN16tQplS9fXk2aNNHmzZvVo0cPR5927dpp9erVmjRpkiZPnqz69etrzZo1atu27Q2fHwAAAFBauPzh7VGjRmnUqFG5fhYbG+u0/+yzz+rZZ5+95pj9+vVTv379iqM8AAAAAAXg0i/IAwAAAHBrIFgAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYc3mwWLx4serWrStPT0+1atVKu3fvzrPv+++/r65du6p69ery8vJSaGiotm7d6tQnNjZWNpstx3bx4sXrPRUAAACg1HJpsFizZo2eeeYZTZw4UYmJierQoYPCwsJ04sSJXPvHx8era9eu2rJliw4cOKBOnTopPDxciYmJTv28vLxkt9udNk9PzxsxJQAAAKBUcnPlyefOnathw4Zp+PDhkqSYmBht3bpVS5Ys0YwZM3L0j4mJcdqfPn26NmzYoE2bNqlly5aOdpvNJj8/v+taOwAAAID/47IVi0uXLunAgQPq1q2bU3u3bt20Z8+eAo2RlZWlCxcuyMfHx6k9PT1dgYGBqlWrlnr16pVjRQMAAABA8XJZsDh9+rSuXLkiX19fp3ZfX18lJycXaIw5c+YoIyND/fv3d7Q1atRIsbGx2rhxo1atWiVPT0+1b99eR44cyXOczMxMpaWlOW0AAAAACs6lt0JJv9+2dDXLsnK05WbVqlWaMmWKNmzYoBo1ajjaQ0JCFBIS4thv37697rrrLi1YsEDz58/PdawZM2YoOjq6iDMAAAAA4LIVi2rVqqls2bI5VidSUlJyrGL80Zo1azRs2DC988476tKlS759y5Qpo9atW+e7YhEVFaXU1FTHdvLkyYJPBAAAAIDrgoW7u7tatWqluLg4p/a4uDi1a9cuz+NWrVqlIUOGaOXKlerZs+c1z2NZlpKSkuTv759nHw8PD3l5eTltAAAAAArOpbdCRUZGKiIiQsHBwQoNDdWyZct04sQJjRw5UtLvKwmnTp3SihUrJP0eKgYNGqR58+YpJCTEsdpRvnx5eXt7S5Kio6MVEhKiBg0aKC0tTfPnz1dSUpIWLVrkmkkCAAAApYBLg8WAAQN05swZTZ06VXa7XU2bNtWWLVsUGBgoSbLb7U7fafHKK6/o8uXLGj16tEaPHu1oHzx4sGJjYyVJ58+f1xNPPKHk5GR5e3urZcuWio+PV5s2bW7o3AAAAIDSxOUPb48aNUqjRo3K9bPssJBt586d1xzv5Zdf1ssvv1wMlQEAAAAoKJd+8zYAAACAWwPBAgAAAIAxggUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgjGABAAAAwBjBAgAAAIAxggUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgjGABAAAAwBjBAgAAAIAxggUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgjGABAAAAwBjBAgAAAIAxggUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgjGABAAAAwBjBAgAAAIAxggUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgjGABAAAAwBjBAgAAAIAxggUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgzM3VBSBvdZ7b7OoSUEDHZ/Z0dQkAAAAuxYoFAAAAAGMECwAAAADGCBYAAAAAjBEsAAAAABgjWAAAAAAwRrAAAAAAYIxgAQAAAMAYwQIAAACAMZcHi8WLF6tu3bry9PRUq1attHv37nz779q1S61atZKnp6fq1aunpUuX5uizdu1aBQUFycPDQ0FBQVq3bt31Kh8AAACAXBws1qxZo2eeeUYTJ05UYmKiOnTooLCwMJ04cSLX/seOHVOPHj3UoUMHJSYm6vnnn9eYMWO0du1aR5+9e/dqwIABioiI0MGDBxUREaH+/fvr888/v1HTAgAAAEodlwaLuXPnatiwYRo+fLgaN26smJgYBQQEaMmSJbn2X7p0qWrXrq2YmBg1btxYw4cP1+OPP67Zs2c7+sTExKhr166KiopSo0aNFBUVpc6dOysmJuYGzQoAAAAofVwWLC5duqQDBw6oW7duTu3dunXTnj17cj1m7969Ofp3795dCQkJ+u233/Ltk9eYAAAAAMy5uerEp0+f1pUrV+Tr6+vU7uvrq+Tk5FyPSU5OzrX/5cuXdfr0afn7++fZJ68xJSkzM1OZmZmO/dTUVElSWlpaoeZU3LIyf3Xp+VFwN/Ja4booObgukBeuDeSG6wJ5ceXvpNnntizrmn1dFiyy2Ww2p33LsnK0Xav/H9sLO+aMGTMUHR2doz0gICDvwoGreMe4ugLcjLgukBeuDeSG6wJ5uRmujQsXLsjb2zvfPi4LFtWqVVPZsmVzrCSkpKTkWHHI5ufnl2t/Nzc3Va1aNd8+eY0pSVFRUYqMjHTsZ2Vl6ezZs6patWq+gQSFk5aWpoCAAJ08eVJeXl6uLgc3Ea4N5IbrAnnh2kBuuC6uD8uydOHCBdWsWfOafV0WLNzd3dWqVSvFxcXpgQcecLTHxcWpd+/euR4TGhqqTZs2ObVt27ZNwcHBKleunKNPXFycxo0b59SnXbt2edbi4eEhDw8Pp7YqVaoUdkooIC8vL/6FR664NpAbrgvkhWsDueG6KH7XWqnI5tJboSIjIxUREaHg4GCFhoZq2bJlOnHihEaOHCnp95WEU6dOacWKFZKkkSNHauHChYqMjNSIESO0d+9eLV++XKtWrXKMOXbsWHXs2FGzZs1S7969tWHDBm3fvl2ffvqpS+YIAAAAlAYuDRYDBgzQmTNnNHXqVNntdjVt2lRbtmxRYGCgJMlutzt9p0XdunW1ZcsWjRs3TosWLVLNmjU1f/589e3b19GnXbt2Wr16tSZNmqTJkyerfv36WrNmjdq2bXvD5wcAAACUFjarII94A8UgMzNTM2bMUFRUVI5bz1C6cW0gN1wXyAvXBnLDdeF6BAsAAAAAxlz6zdsAAAAAbg0ECwAAAADGCBYAAAAAjBEscN3Fx8crPDxcNWvWlM1m0/r1611dEm4CM2bMUOvWrVW5cmXVqFFDffr00eHDh11dFm4CS5YsUfPmzR3vog8NDdWHH37o6rJwk5kxY4ZsNpueeeYZV5cCF5syZYpsNpvT5ufn5+qySiWCBa67jIwMtWjRQgsXLnR1KbiJ7Nq1S6NHj9a+ffsUFxeny5cvq1u3bsrIyHB1aXCxWrVqaebMmUpISFBCQoLuu+8+9e7dW99++62rS8NNYv/+/Vq2bJmaN2/u6lJwk2jSpInsdrtj+/rrr11dUqnk0u+xQOkQFhamsLAwV5eBm8xHH33ktP/GG2+oRo0aOnDggDp27OiiqnAzCA8Pd9r/5z//qSVLlmjfvn1q0qSJi6rCzSI9PV2PPfaYXn31VU2bNs3V5eAm4ebmxirFTYAVCwA3hdTUVEmSj4+PiyvBzeTKlStavXq1MjIyFBoa6upycBMYPXq0evbsqS5duri6FNxEjhw5opo1a6pu3bp6+OGH9d///tfVJZVKrFgAcDnLshQZGam7775bTZs2dXU5uAl8/fXXCg0N1cWLF1WpUiWtW7dOQUFBri4LLrZ69Wp9+eWX2r9/v6tLwU2kbdu2WrFihe644w79/PPPmjZtmtq1a6dvv/1WVatWdXV5pQrBAoDLPfXUU/rqq6/06aefuroU3CQaNmyopKQknT9/XmvXrtXgwYO1a9cuwkUpdvLkSY0dO1bbtm2Tp6enq8vBTeTq262bNWum0NBQ1a9fX2+++aYiIyNdWFnpQ7AA4FJPP/20Nm7cqPj4eNWqVcvV5eAm4e7urttvv12SFBwcrP3792vevHl65ZVXXFwZXOXAgQNKSUlRq1atHG1XrlxRfHy8Fi5cqMzMTJUtW9aFFeJmUbFiRTVr1kxHjhxxdSmlDsECgEtYlqWnn35a69at086dO1W3bl1Xl4SbmGVZyszMdHUZcKHOnTvneNPP0KFD1ahRI02YMIFQAYfMzEx999136tChg6tLKXUIFrju0tPTdfToUcf+sWPHlJSUJB8fH9WuXduFlcGVRo8erZUrV2rDhg2qXLmykpOTJUne3t4qX768i6uDKz3//PMKCwtTQECALly4oNWrV2vnzp053iSG0qVy5co5nsGqWLGiqlatyrNZpdz48eMVHh6u2rVrKyUlRdOmTVNaWpoGDx7s6tJKHYIFrruEhAR16tTJsZ99v+PgwYMVGxvroqrgakuWLJEk3XvvvU7tb7zxhoYMGXLjC8JN4+eff1ZERITsdru8vb3VvHlzffTRR+rataurSwNwE/rxxx/1yCOP6PTp06pevbpCQkK0b98+BQYGurq0UsdmWZbl6iIAAAAAlGx8jwUAAAAAYwQLAAAAAMYIFgAAAACMESwAAAAAGCNYAAAAADBGsAAAAABgjGABAAAAwBjBAgAAAIAxggUAAMWsTp06iomJcXUZAHBDESwA4BYyZMgQ2Ww2zZw506l9/fr1stlsBRojMTFRAwYMkL+/vzw8PBQYGKhevXpp06ZNsizrepRdJDfDL++xsbGqUqWKS2sAgJsFwQIAbjGenp6aNWuWzp07V+hjN2zYoJCQEKWnp+vNN9/Uv//9b7377rvq06ePJk2apNTU1FyPsyxLly9fNi0dAFCCESwA4BbTpUsX+fn5acaMGYU6LiMjQ8OGDVPPnj21efNmdevWTfXr11ebNm00fPhwHTx4UN7e3pKknTt3ymazaevWrQoODpaHh4d2796t77//Xr1795avr68qVaqk1q1ba/v27U7nqVOnjqZNm6ZBgwapUqVKCgwM1IYNG/TLL7+od+/eqlSpkpo1a6aEhASjn8OmTZvUqlUreXp6ql69eoqOjnYKPzabTa+99poeeOABVahQQQ0aNNDGjRudxti4caMaNGig8uXLq1OnTnrzzTdls9l0/vx57dy5U0OHDlVqaqpsNptsNpumTJniOPbXX3/V448/rsqVK6t27dpatmyZ0XwA4GZHsACAW0zZsmU1ffp0LViwQD/++GOBj9u2bZvOnDmjZ599Ns8+f7yd6tlnn9WMGTP03XffqXnz5kpPT1ePHj20fft2JSYmqnv37goPD9eJEyecjnv55ZfVvn17JSYmqmfPnoqIiNCgQYM0cOBAffnll7r99ts1aNCgIt96tXXrVg0cOFBjxozRv//9b73yyiuKjY3VP//5T6d+0dHR6t+/v7766iv16NFDjz32mM6ePStJOn78uPr166c+ffooKSlJf/nLXzRx4kTHse3atVNMTIy8vLxkt9tlt9s1fvx4x+dz5sxRcHCwEhMTNWrUKD355JM6dOhQkeYDACWCBQC4ZQwePNjq3bu3ZVmWFRISYj3++OOWZVnWunXrrGv9J3/mzJmWJOvs2bOOti+++MKqWLGiY9u0aZNlWZb1ySefWJKs9evXX7OmoKAga8GCBY79wMBAa+DAgY59u91uSbImT57saNu7d68lybLb7XmOGxgYaL388su5ftahQwdr+vTpTm1vvfWW5e/v79iXZE2aNMmxn56ebtlsNuvDDz+0LMuyJkyYYDVt2tRpjIkTJ1qSrHPnzlmWZVlvvPGG5e3tnWttV88xKyvLqlGjhrVkyZI85wMAJR0rFgBwi5o1a5bjOYk/qlSpkmMbOXJknmM0b95cSUlJSkpKUkZGRo7nKIKDg532MzIy9OyzzyooKEhVqlRRpUqVdOjQoRwrFs2bN3f82dfXV5LUrFmzHG0pKSkFnK2zAwcOaOrUqU7zHDFihOx2u3799ddc66hYsaIqV67sOOfhw4fVunVrp3HbtGlT4BquHttms8nPz6/I8wGAksDN1QUAAK6Pjh07qnv37nr++ec1ZMgQp8+SkpIcf/by8pIkNWjQQNLvv1CHhIRIkjw8PHT77bfneY6KFSs67f/tb3/T1q1bNXv2bN1+++0qX768+vXrp0uXLjn1K1eunOPP2bdX5daWlZVVkKnmkJWVpejoaD344IM5PvP09My1juzzZp/Tsqwct35Zhbg1K7+xAeBWRLAAgFvYzJkzdeedd+qOO+5was8tLHTr1k0+Pj6aNWuW1q1bV6Tz7d69W0OGDNEDDzwgSUpPT9fx48eLNJaJu+66S4cPH843FF1Lo0aNtGXLFqe2Pz5Q7u7uritXrhT5HABwKyFYAMAtrFmzZnrssce0YMGCa/atVKmSXnvtNQ0YMEA9e/bUmDFj1KBBA6Wnp+ujjz6S9PuD4fm5/fbb9f777ys8PFw2m02TJ0++rn9Lf+rUKafVF0mqXbu2/v73v6tXr14KCAjQQw89pDJlyuirr77S119/rWnTphVo7L/85S+aO3euJkyYoGHDhikpKUmxsbGS/m9FpU6dOkpPT9fHH3+sFi1aqEKFCqpQoUJxThEASgyesQCAW9w//vGPAt/C88ADD2jPnj2qUKGCBg0apIYNG+q+++7Tjh07tHr1avXq1Svf419++WXddtttateuncLDw9W9e3fdddddxTGNXM2ePVstW7Z02jZu3Kju3bvrgw8+UFxcnFq3bq2QkBDNnTtXgYGBBR67bt26eu+99/T++++refPmWrJkieOtUB4eHpJ+fzPUyJEjNWDAAFWvXl0vvvjidZknAJQENqswN4wCAFCK/fOf/9TSpUt18uRJV5cCADcdboUCACAPixcvVuvWrVW1alV99tlneumll/TUU0+5uiwAuCkRLAAAyMORI0c0bdo0nT17VrVr19Zf//pXRUVFubosALgpcSsUAAAAAGM8vA0AAADAGMECAAAAgDGCBQAAAABjBAsAAAAAxggWAAAAAIwRLAAAAAAYI1gAAAAAMEawAAAAAGCMYAEAAADA2P8D4QptHMNqH94AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot of n-gram counts \n",
    "# Extracting n-gram statistics \n",
    "ngram_stats = { \n",
    "    n: len(context_sensetive_corrector.ngram_dicts[n])\n",
    "    for n in range(1, 6)\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.bar(ngram_stats.keys(), ngram_stats.values())\n",
    "ax.set_title(\"N-Gram Counts (1-5 Grams)\")\n",
    "ax.set_xlabel(\"N-Gram Length\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:12.163495Z",
     "iopub.status.busy": "2025-02-25T22:31:12.163020Z",
     "iopub.status.idle": "2025-02-25T22:31:12.170400Z",
     "shell.execute_reply": "2025-02-25T22:31:12.169717Z",
     "shell.execute_reply.started": "2025-02-25T22:31:12.163460Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: speling\n",
      "Corrected: spelling\n",
      "--------------------------------------------------\n",
      "Original: dking\n",
      "Corrected: doing\n",
      "--------------------------------------------------\n",
      "Original: fel\n",
      "Corrected: few\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_words = [\n",
    "    \"speling\",\n",
    "    \"dking\",\n",
    "    \"fel\"\n",
    "]\n",
    "\n",
    "for word in test_words:\n",
    "    corrected_word = context_sensetive_corrector.correct_word(word)\n",
    "    print(f\"Original: {word}\")\n",
    "    print(f\"Corrected: {corrected_word}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:12.171941Z",
     "iopub.status.busy": "2025-02-25T22:31:12.171695Z",
     "iopub.status.idle": "2025-02-25T22:31:12.498888Z",
     "shell.execute_reply": "2025-02-25T22:31:12.497949Z",
     "shell.execute_reply.started": "2025-02-25T22:31:12.171919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: a bad cas of the\n",
      "Corrected: a bad case of the\n",
      "--------------------------------------------------\n",
      "Original: He fel in a puddle.\n",
      "Corrected: he fell in a puddle .\n",
      "--------------------------------------------------\n",
      "Original: He did not fel well.\n",
      "Corrected: he did not feel well .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"a bad cas of the\",\n",
    "    \"He fel in a puddle.\",\n",
    "    \"He did not fel well.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    corrected_sentence = context_sensetive_corrector.correct_sentence(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Corrected: {corrected_sentence}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46rk65S4GRSe"
   },
   "source": [
    "## 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norvig's corrector (from https://norvig.com/spell-correct.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:19.617657Z",
     "iopub.status.busy": "2025-02-25T22:31:19.617311Z",
     "iopub.status.idle": "2025-02-25T22:31:20.007339Z",
     "shell.execute_reply": "2025-02-25T22:31:20.006355Z",
     "shell.execute_reply.started": "2025-02-25T22:31:19.617629Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class NorwigCorrector:\n",
    "    def __init__(self, corpus_path):\n",
    "        self.unigrams = Counter(self.words(open(corpus_path).read()))\n",
    "\n",
    "    def words(self, text):\n",
    "        return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "    def P(self, word):\n",
    "        \"\"\"Probability of `word`.\"\"\"\n",
    "        N = sum(self.unigrams.values())\n",
    "        return self.unigrams[word] / N\n",
    "\n",
    "    def correct_word(self, word):\n",
    "        \"\"\"Most probable spelling correction for word.\"\"\"\n",
    "        return max(self.candidates(word), key=self.P)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        \"\"\"Generate possible spelling corrections for word.\"\"\"\n",
    "        return self.known([word]) or self.known(self.edits1(word)) or self.known(self.edits2(word)) or [word]\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"The subset of `words` that appear in the dictionary of unigrams.\"\"\"\n",
    "        return set(w for w in words if w in self.unigrams)\n",
    "\n",
    "    def edits1(self, word):\n",
    "        \"\"\"All edits that are one edit away from `word`.\"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"All edits that are two edits away from `word`.\"\"\"\n",
    "        return set(e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n",
    "\n",
    "    # new function to correct words in sentences\n",
    "    def correct_sentence(self, sentence):\n",
    "        \"\"\"Corrects a given sentence by applying spelling correction to each word.\"\"\"\n",
    "        return ' '.join(self.correct_word(word) for word in sentence.split())\n",
    "\n",
    "norwig_corrector = NorwigCorrector('brown_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:20.008969Z",
     "iopub.status.busy": "2025-02-25T22:31:20.008603Z",
     "iopub.status.idle": "2025-02-25T22:31:20.024074Z",
     "shell.execute_reply": "2025-02-25T22:31:20.023133Z",
     "shell.execute_reply.started": "2025-02-25T22:31:20.008934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: speling\n",
      "Corrected: spelling\n",
      "--------------------------------------------------\n",
      "Original: dking\n",
      "Corrected: doing\n",
      "--------------------------------------------------\n",
      "Original: fel\n",
      "Corrected: few\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_words = [\n",
    "    \"speling\",\n",
    "    \"dking\",\n",
    "    \"fel\"\n",
    "]\n",
    "\n",
    "for word in test_words:\n",
    "    corrected_word = norwig_corrector.correct_word(word)\n",
    "    print(f\"Original: {word}\")\n",
    "    print(f\"Corrected: {corrected_word}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:20.949115Z",
     "iopub.status.busy": "2025-02-25T22:31:20.948750Z",
     "iopub.status.idle": "2025-02-25T22:31:21.000514Z",
     "shell.execute_reply": "2025-02-25T22:31:20.999192Z",
     "shell.execute_reply.started": "2025-02-25T22:31:20.949087Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: a bad cas of the\n",
      "Corrected: a bad was of the\n",
      "--------------------------------------------------\n",
      "Original: He fel in a puddle.\n",
      "Corrected: he few in a puddles\n",
      "--------------------------------------------------\n",
      "Original: He did not fel well.\n",
      "Corrected: he did not few well\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"a bad cas of the\",\n",
    "    \"He fel in a puddle.\",\n",
    "    \"He did not fel well.\"\n",
    "]\n",
    "\n",
    "for sentence in test_sentences:\n",
    "    corrected_sentence = norwig_corrector.correct_sentence(sentence)\n",
    "    print(f\"Original: {sentence}\")\n",
    "    print(f\"Corrected: {corrected_sentence}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating correctors on word misspellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:23.147832Z",
     "iopub.status.busy": "2025-02-25T22:31:23.147457Z",
     "iopub.status.idle": "2025-02-25T22:31:23.154730Z",
     "shell.execute_reply": "2025-02-25T22:31:23.153849Z",
     "shell.execute_reply.started": "2025-02-25T22:31:23.147803Z"
    },
    "id": "OwZWaX9VVs7B",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def spell_correction_test_for_words(file_path, corrector, verbose=False):\n",
    "    \"\"\"\n",
    "    Reads a test set from a file and evaluates the spelling correction model on individual words.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the test file (each line in 'correct_word: wrong1 wrong2' format)\n",
    "    - corrector: Spelling correction model instance (must have a 'correct_word' method)\n",
    "    - verbose: If True, outputs detailed correction information\n",
    "\n",
    "    Outputs:\n",
    "    - Displays percentage of correctly fixed words, execution speed per word.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    # Read test cases from file and parse them into (correct_word, misspelled_word) pairs\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        tests = [(right.strip(), wrong.strip())\n",
    "                 for line in file if ':' in line\n",
    "                 for right, wrongs in [line.strip().split(':')]\n",
    "                 for wrong in wrongs.strip().split()]\n",
    "\n",
    "    start = time.time()\n",
    "    good, unknown = 0, 0\n",
    "    n = len(tests)\n",
    "\n",
    "    vocab = corrector.unigrams \n",
    "\n",
    "    for right, wrong in tests:\n",
    "        w = corrector.correct_word(wrong)\n",
    "        good += (w == right)\n",
    "        if w != right:\n",
    "            unknown += (right not in vocab)\n",
    "            \n",
    "            if verbose:\n",
    "                print(f'correction({wrong}) => {w} ({vocab.get(w, 0)}); expected {right} ({vocab.get(right, 0)})')\n",
    "\n",
    "    dt = time.time() - start\n",
    "    word_accuracy = good / n\n",
    "\n",
    "     # Reports efficiency of correction model\n",
    "    print(f'Accuracy: {word_accuracy:.2%}')\n",
    "    print(f'Processed {n} words in {dt:.2f} seconds')\n",
    "    print(f'Processing Speed: {n / dt:.2f} words per second')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norvig's corrector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:25.446917Z",
     "iopub.status.busy": "2025-02-25T22:31:25.446543Z",
     "iopub.status.idle": "2025-02-25T22:31:35.555594Z",
     "shell.execute_reply": "2025-02-25T22:31:35.554694Z",
     "shell.execute_reply.started": "2025-02-25T22:31:25.446888Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.22%\n",
      "Processed 270 words in 6.12 seconds\n",
      "Processing Speed: 44.08 words per second\n"
     ]
    }
   ],
   "source": [
    "# Development set \n",
    "spell_correction_test_for_words(\"spell-testset1.txt\", norwig_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:31:35.556966Z",
     "iopub.status.busy": "2025-02-25T22:31:35.556629Z",
     "iopub.status.idle": "2025-02-25T22:31:51.445777Z",
     "shell.execute_reply": "2025-02-25T22:31:51.444857Z",
     "shell.execute_reply.started": "2025-02-25T22:31:35.556932Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.50%\n",
      "Processed 400 words in 9.94 seconds\n",
      "Processing Speed: 40.25 words per second\n"
     ]
    }
   ],
   "source": [
    "# Final test set\n",
    "spell_correction_test_for_words(\"spell-testset2.txt\", norwig_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context sensetive corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:32:03.414933Z",
     "iopub.status.busy": "2025-02-25T22:32:03.414508Z",
     "iopub.status.idle": "2025-02-25T22:32:12.895128Z",
     "shell.execute_reply": "2025-02-25T22:32:12.894108Z",
     "shell.execute_reply.started": "2025-02-25T22:32:03.414900Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.33%\n",
      "Processed 270 words in 6.05 seconds\n",
      "Processing Speed: 44.63 words per second\n"
     ]
    }
   ],
   "source": [
    "# Development set\n",
    "spell_correction_test_for_words(\"spell-testset1.txt\", context_sensetive_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:32:12.896643Z",
     "iopub.status.busy": "2025-02-25T22:32:12.896310Z",
     "iopub.status.idle": "2025-02-25T22:32:28.309547Z",
     "shell.execute_reply": "2025-02-25T22:32:28.308379Z",
     "shell.execute_reply.started": "2025-02-25T22:32:12.896608Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 71.50%\n",
      "Processed 400 words in 10.24 seconds\n",
      "Processing Speed: 39.07 words per second\n"
     ]
    }
   ],
   "source": [
    "# Final test set\n",
    "spell_correction_test_for_words(\"spell-testset2.txt\", context_sensetive_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating correctors on word misspellings within sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Realistic Test Data with `sage`**\n",
    "\n",
    "To generate a **realistic test dataset** for spell correction, I used **misspelling statistics** from [`ai-forever/sage`](https://github.com/ai-forever/sage.git). This library provides **real-world typo distributions** based on phonetic, keyboard, and common spelling mistakes, making the test data more reflective of actual user errors. By leveraging these statistics, we can create **synthetic misspelled words in sentences** to evaluate and fine-tune spell correction model effectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:32:37.610969Z",
     "iopub.status.busy": "2025-02-25T22:32:37.610591Z",
     "iopub.status.idle": "2025-02-25T22:32:37.618785Z",
     "shell.execute_reply": "2025-02-25T22:32:37.617721Z",
     "shell.execute_reply.started": "2025-02-25T22:32:37.610941Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def corrupt_sentences_with_sage(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Generates a corrupted dataset using real-world misspelling patterns from SAGE.\n",
    "\n",
    "    This function takes a clean text dataset and introduces realistic spelling errors\n",
    "    based on statistical models from the SAGE library. It ensures that common typos\n",
    "    (phonetic, keyboard, and spelling errors) are incorporated to create a robust test set.\n",
    "\n",
    "    Parameters:\n",
    "    - input_file_path: Path to the original clean dataset (each line is a sentence).\n",
    "    - output_file_path: Path where the corrupted dataset will be saved.\n",
    "\n",
    "    The function:\n",
    "    - Checks if the corrupted dataset already exists to prevent redundant processing.\n",
    "    - Automatically installs SAGE if missing.\n",
    "    - Uses SAGE’s spelling corruption models to generate realistic typos.\n",
    "    - Saves the corrupted and original sentences in a CSV file for evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    if os.path.exists(output_file_path):\n",
    "        print(f\"Corrupted dataset already exists: {output_file_path}\")\n",
    "        return\n",
    "\n",
    "    if not os.path.exists(input_file_path):\n",
    "        print(f\"File {input_file_path} not found! Upload it before running.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {input_file_path}, processing...\")\n",
    "\n",
    "    try:\n",
    "        from sage.spelling_corruption import SBSCConfig, SBSCCorruptor\n",
    "    except ImportError:\n",
    "        print(\"SAGE not found, installing...\")\n",
    "        os.system(\"git clone https://github.com/ai-forever/sage.git\")\n",
    "        os.chdir(\"sage\")\n",
    "        os.system(\"pip install -e '.[errant]' --quiet\")\n",
    "        from sage.spelling_corruption import SBSCConfig, SBSCCorruptor\n",
    "\n",
    "    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = [line.strip() for line in file.readlines() if line.strip()]\n",
    "\n",
    "    print(f\"Creating corrupted dataset: {output_file_path}\")\n",
    "\n",
    "    # Configure the corruptor to introduce realistic spelling errors\n",
    "    config = SBSCConfig(\n",
    "        lang=\"en\",\n",
    "        reference_dataset_name_or_path=os.path.join(\"data\", \"example_data\", \"jfleg\"),\n",
    "    )\n",
    "\n",
    "    corruptor = SBSCCorruptor.from_config(config)\n",
    "    corrupted_lines = corruptor.batch_corrupt(lines)\n",
    "\n",
    "    # Save the corrupted sentences alongside their correct versions for evaluation\n",
    "    df = pd.DataFrame({\"corrupted_text\": corrupted_lines, \"corrected_text\": lines})\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "\n",
    "    print(f\"Corrupted dataset saved as {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:32:43.941146Z",
     "iopub.status.busy": "2025-02-25T22:32:43.940783Z",
     "iopub.status.idle": "2025-02-25T22:32:43.945823Z",
     "shell.execute_reply": "2025-02-25T22:32:43.944845Z",
     "shell.execute_reply.started": "2025-02-25T22:32:43.941118Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted dataset already exists: brown_dev_test_corrupted_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a corrupted version of the Brown development test set using SAGE\n",
    "corrupt_sentences_with_sage(\"brown_dev_test.txt\", \"brown_dev_test_corrupted_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:32:44.396462Z",
     "iopub.status.busy": "2025-02-25T22:32:44.396078Z",
     "iopub.status.idle": "2025-02-25T22:32:44.401111Z",
     "shell.execute_reply": "2025-02-25T22:32:44.400077Z",
     "shell.execute_reply.started": "2025-02-25T22:32:44.396420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted dataset already exists: brown_test_corrupted_sentences.txt\n"
     ]
    }
   ],
   "source": [
    "# Create a corrupted version of the Brown test set using SAGE\n",
    "corrupt_sentences_with_sage(\"brown_test.txt\", \"brown_test_corrupted_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T23:10:27.111515Z",
     "iopub.status.busy": "2025-02-25T23:10:27.110648Z",
     "iopub.status.idle": "2025-02-25T23:10:27.120861Z",
     "shell.execute_reply": "2025-02-25T23:10:27.119789Z",
     "shell.execute_reply.started": "2025-02-25T23:10:27.111477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def spell_correction_test_for_sentences(file_path, corrector, verbose=False):\n",
    "    \"\"\"\n",
    "    Evaluates a spelling correction model on full sentences using a test data.\n",
    "\n",
    "    This function reads a CSV file containing corrupted and corrected sentences, \n",
    "    applies the spell corrector, and compares the predicted corrections against the ground truth. \n",
    "    It calculates the token-level accuracy and measures processing speed.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path: Path to the CSV file with 'corrupted_text' and 'corrected_text' columns.\n",
    "    - corrector: Spelling correction model instance (must have 'correct_sentence' method).\n",
    "    - verbose: If True, outputs detailed correction information.\n",
    "\n",
    "    Outputs:\n",
    "    - Accuracy of word-level corrections across all sentences.\n",
    "    - Processing speed in words per second and sentences per second.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    required_cols = {\"corrupted_text\", \"corrected_text\"}\n",
    "    if not required_cols.issubset(df.columns):\n",
    "        raise ValueError(f\"CSV file must contain columns: {required_cols}\")\n",
    "\n",
    "    df = df.dropna(subset=[\"corrupted_text\", \"corrected_text\"])\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    \n",
    "    start = time.time()\n",
    "    total_tokens, count = 0, 0\n",
    "\n",
    "    # Iterates through each sentence in the dataset and applies the correction model\n",
    "    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing Sentences\"):\n",
    "        typo_sentence = row[\"corrupted_text\"]\n",
    "        correct_sentence = row[\"corrected_text\"]\n",
    "        \n",
    "        fixed_sentence = corrector.correct_sentence(typo_sentence)\n",
    "\n",
    "        # Tokenizes the ground truth and corrected sentence for comparison\n",
    "        correct_tokens = tokenizer.tokenize(correct_sentence.lower())\n",
    "        fixed_tokens = tokenizer.tokenize(fixed_sentence.lower())\n",
    "\n",
    "        if verbose:\n",
    "            print(correct_tokens)\n",
    "            print(fixed_tokens)\n",
    "            print([(correct_tokens[i], fixed_tokens[i]) for i in range(min(len(correct_tokens), len(fixed_tokens))) \n",
    "                   if correct_tokens[i] != fixed_tokens[i]])\n",
    "\n",
    "        total_tokens += len(correct_tokens)\n",
    "\n",
    "        # Counts how many words were correctly restored\n",
    "        tmp_count = sum(1 for i in range(min(len(correct_tokens), len(fixed_tokens))) \n",
    "                        if correct_tokens[i] == fixed_tokens[i])\n",
    "        count += tmp_count\n",
    "\n",
    "        if verbose and total_tokens > 0:\n",
    "            print(f'Correct: {correct_sentence}')\n",
    "            print(f'With error:  {typo_sentence}')\n",
    "            print(f'Corrected: {fixed_sentence}')\n",
    "            print(f'Accuracy: {tmp_count}/{len(correct_tokens)}')\n",
    "            print('-' * 50)\n",
    "\n",
    "    dt = time.time() - start\n",
    "\n",
    "    # Computes overall accuracy and processing speed\n",
    "    accuracy = count / total_tokens if total_tokens else 0\n",
    "    print(f'\\nAccuracy: {accuracy:.2%}')\n",
    "    print(f'Processed {len(df)} sentences in {dt:.2f} seconds')\n",
    "    print(f'Processing Speed: {total_tokens / dt:.2f} words/sec, {len(df) / dt:.2f} sentences/sec')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Norvig's corrector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:46:16.935788Z",
     "iopub.status.busy": "2025-02-25T22:46:16.934912Z",
     "iopub.status.idle": "2025-02-25T22:46:30.695168Z",
     "shell.execute_reply": "2025-02-25T22:46:30.694314Z",
     "shell.execute_reply.started": "2025-02-25T22:46:16.935717Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 2867/2867 [04:28<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 75.27%\n",
      "Processed 2867 sentences in 268.48 seconds\n",
      "Processing Speed: 216.90 words/sec, 10.68 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Development set \n",
    "spell_correction_test_for_sentences(\"brown_dev_test_corrupted_sentences.txt\", norwig_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:46:30.696718Z",
     "iopub.status.busy": "2025-02-25T22:46:30.696453Z",
     "iopub.status.idle": "2025-02-25T22:47:01.428723Z",
     "shell.execute_reply": "2025-02-25T22:47:01.427739Z",
     "shell.execute_reply.started": "2025-02-25T22:46:30.696694Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 8601/8601 [13:22<00:00, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 75.17%\n",
      "Processed 8601 sentences in 802.16 seconds\n",
      "Processing Speed: 219.57 words/sec, 10.72 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Final test set\n",
    "spell_correction_test_for_sentences(\"brown_test_corrupted_sentences.txt\", norwig_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Context sensetive corrector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T23:09:11.793833Z",
     "iopub.status.busy": "2025-02-25T23:09:11.793445Z",
     "iopub.status.idle": "2025-02-25T23:09:56.488420Z",
     "shell.execute_reply": "2025-02-25T23:09:56.487122Z",
     "shell.execute_reply.started": "2025-02-25T23:09:11.793798Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 2867/2867 [16:43<00:00,  2.86it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 97.19%\n",
      "Processed 2867 sentences in 1003.25 seconds\n",
      "Processing Speed: 58.05 words/sec, 2.86 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Development set\n",
    "spell_correction_test_for_sentences(\"brown_dev_test_corrupted_sentences.txt\", context_sensetive_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-25T22:47:49.630552Z",
     "iopub.status.busy": "2025-02-25T22:47:49.630182Z",
     "iopub.status.idle": "2025-02-25T22:49:11.699102Z",
     "shell.execute_reply": "2025-02-25T22:49:11.698071Z",
     "shell.execute_reply.started": "2025-02-25T22:47:49.630518Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences: 100%|██████████| 8601/8601 [52:49<00:00,  2.71it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy: 97.12%\n",
      "Processed 8601 sentences in 3169.46 seconds\n",
      "Processing Speed: 55.57 words/sec, 2.71 sentences/sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Final test set\n",
    "spell_correction_test_for_sentences(\"brown_test_corrupted_sentences.txt\", context_sensetive_corrector, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "I evaluated two spell correction models on various test datasets, focusing on word-level and sentence-level corrections. All results here are based on the final test set.\n",
    "\n",
    "### Word-Level Evaluation\n",
    "\n",
    "The word-level evaluation focuses on correcting individual words. The following table presents the accuracy of each model on the final test set.\n",
    "\n",
    "| Model                    | Final Test Set Accuracy | Processed Words | Time (Seconds) | Processing Speed (words/sec) |\n",
    "|--------------------------|-------------------------|-----------------|----------------|-----------------------------|\n",
    "| Norvig Corrector           | 73.50%                  | 400             | 9.94           | 40.25                       |\n",
    "| Context-Sensitive Corrector                | 71.50%                  | 400             | 10.24          | 39.07                       |\n",
    "\n",
    "The **Norvig Corrector** achieves a slightly higher accuracy compared to **Context-Sensitive Corrector**. However, the processing speed for both models is relatively similar, with Norvig’s corrector processing words a bit faster.\n",
    "\n",
    "### Sentence-Level Evaluation\n",
    "\n",
    "For sentence-level evaluation, I evaluated how well each model can correct sentences with multiple typos. The accuracy of corrections was calculated based on token-level matches between the ground truth and the corrected sentences. The table below shows the results of this evaluation.\n",
    "\n",
    "| Model                    | Final Test Set Accuracy | Processed Sentences | Time (Seconds) | Processing Speed (sentences/sec) |\n",
    "|--------------------------|-------------------------|---------------------|----------------|---------------------------------|\n",
    "| Norvig Corrector           | 75.17%                  | 8601                | 802.16         | 10.72                           |\n",
    "| Context-Sensitive Corrector  | 97.12%                  | 8601                | 3169.46        | 2.71                            |\n",
    "\n",
    "The **Context-Sensitive Corrector** demonstrates a much higher accuracy at the sentence level, although it processes sentences slower than Norvig's model, likely due to the more complex correction approach employed by **Context-Sensitive Corrector**.\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, **Norvig Corrector** performs better in terms of speed, both at the word and sentence levels. However, **Context-Sensitive Corrector** significantly outperforms **Norvig Corrector** in terms of accuracy, especially at the sentence level. The trade-off between processing speed and accuracy should be considered based on the application requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future work\n",
    "\n",
    "There are several promising directions that can build upon this work and improve spell correction models:\n",
    "\n",
    "### 1. Enhancing N-gram-Based Corrections with Interpolation  \n",
    "One possible improvement is incorporating **interpolation techniques** from [Jurafsky & Martin’s Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/3.pdf). Interpolation allows the model to combine **higher-order n-grams (e.g., trigrams, 4-grams) with lower-order ones** (e.g., unigrams, bigrams) to mitigate **data sparsity issues** and improve correction accuracy. By applying **linear interpolation strategies**, this approach could lead to more reliable word predictions in cases where n-grams are missing or underrepresented in the dataset. Other methods are also can be considered to improve the solution.\n",
    "\n",
    "### 2. Exploring Grammar Correction as a Related Task  \n",
    "While this work focused on **spelling correction**, similar methods might be useful for **grammar correction** as well. Since grammatical errors often involve **misplaced or incorrect words**, n-gram probability models and ranking methods could potentially correct **both spelling and grammar mistakes**. Testing this approach on datasets like the [Kaggle Grammar Correction Dataset](https://www.kaggle.com/datasets/satishgunjal/grammar-correction) would provide valuable insights into its feasibility.\n",
    "\n",
    "### 3. Evaluating on Real-Life Multilingual Datasets  \n",
    "To ensure broader applicability, this approach can be trained and tested on **real-world misspelling datasets** in different languages:\n",
    "- **Croatian:** The [Ispravi-Me dataset](https://github.com/Ispravi-Me/Dataset-of-Misspelings-and-Corrections) contains naturally occurring spelling errors and corrections.\n",
    "- **Russian:** The [AI-Forever Spellcheck Benchmark](https://huggingface.co/datasets/ai-forever/spellcheck_benchmark) provides real-life Russian misspellings.  \n",
    "Applying this method to different languages would help evaluate its generalizability and uncover language-specific error patterns.\n",
    "\n",
    "### 4. Creating a High-Quality Benchmark for English Spelling Correction  \n",
    "There are many multilingual datasets available, but a **strong benchmark for English spelling correction** is still lacking. By leveraging **real-world typo distributions and user-generated errors**, a high-quality dataset could be created to:\n",
    "- **Evaluate and compare** different spell correction methods fairly.\n",
    "- Provide a **standardized test set** for future research.\n",
    "- Reflect **realistic typing and spelling errors**, making corrections more useful for practical applications.\n",
    "\n",
    "Each of these areas represents a separate yet **complementary direction** for improving spell correction, from **algorithmic refinements to multilingual adaptability and dataset creation**.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6722198,
     "sourceId": 10854251,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
